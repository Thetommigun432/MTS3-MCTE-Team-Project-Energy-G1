{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd32b5d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7ad86fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:\\Users\\Tommaso\\Documents\\MEGAR2D2\\HOWEST\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\n",
      "Data directory: C:\\Users\\Tommaso\\Documents\\MEGAR2D2\\HOWEST\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\n",
      "Output directory: C:\\Users\\Tommaso\\Documents\\MEGAR2D2\\HOWEST\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\\model_ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('.').resolve().parent\n",
    "DATA_DIR = BASE_DIR / 'data' / 'processed' / '15min'\n",
    "OUTPUT_DIR = DATA_DIR / 'model_ready'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2efc7123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 35040 rows, 20 columns\n",
      "Time range: 2024-10-20 02:15:00 → 2025-10-20 02:00:00\n",
      "\n",
      "Columns: ['Time', 'Aggregate', 'RangeHood', 'Dryer', 'Stove', 'GarageCabinet', 'ChargingStation_Socket', 'Oven', 'RainwaterPump', 'SmappeeCharger', 'Dishwasher', 'HeatPump', 'HeatPump_Controller', 'WashingMachine', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "input_file = DATA_DIR / 'nilm_ready_dataset.parquet'\n",
    "if not input_file.exists():\n",
    "    input_file = DATA_DIR / 'nilm_ready_dataset.csv'\n",
    "\n",
    "if input_file.suffix == '.parquet':\n",
    "    df = pd.read_parquet(input_file)\n",
    "else:\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df.sort_values('Time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"Time range: {df['Time'].min()} → {df['Time'].max()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80906fbd",
   "metadata": {},
   "source": [
    "## 1. Split (Block Time-Series Interleaved)\n",
    "\n",
    "**Why Block Interleaved?**\n",
    "- Sequential split (e.g., first 8 months Train) would miss winter patterns in Test\n",
    "- Random shuffle causes data leakage in time-series\n",
    "- Block interleaved distributes all seasons across all sets\n",
    "\n",
    "**Configuration:**\n",
    "- Block size: 7 days (672 samples at 15-min resolution)\n",
    "- Pattern: [Train, Train, Train, Train, Val, Test] (modulo 6)\n",
    "- Result: ~66% Train, ~16% Val, ~16% Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b3a30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_time_series_split(\n",
    "    df: pd.DataFrame,\n",
    "    block_days: int = 7,\n",
    "    pattern: list = None,\n",
    "    time_column: str = 'Time'\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Split time-series data into Train/Val/Test using interleaved blocks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with time column\n",
    "    block_days : int\n",
    "        Number of days per block (default: 7 for weekly blocks)\n",
    "    pattern : list\n",
    "        Assignment pattern. Default: [0,0,0,0,1,2] where 0=Train, 1=Val, 2=Test\n",
    "    time_column : str\n",
    "        Name of the time column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    if pattern is None:\n",
    "        pattern = [0, 0, 0, 0, 1, 2]  # ~66% Train, ~16% Val, ~16% Test\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "    df = df.sort_values(time_column).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate block size (15-min resolution: 96 samples/day)\n",
    "    samples_per_day = 24 * 4\n",
    "    block_size = block_days * samples_per_day\n",
    "    \n",
    "    # Assign block number and set\n",
    "    df['_block'] = df.index // block_size\n",
    "    df['_set'] = df['_block'].apply(lambda x: pattern[x % len(pattern)])\n",
    "    \n",
    "    # Split\n",
    "    train_df = df[df['_set'] == 0].drop(columns=['_block', '_set'])\n",
    "    val_df = df[df['_set'] == 1].drop(columns=['_block', '_set'])\n",
    "    test_df = df[df['_set'] == 2].drop(columns=['_block', '_set'])\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec8364c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set              Rows        %\n",
      "------------------------------\n",
      "Train           24192    69.0%\n",
      "Validation       5472    15.6%\n",
      "Test             5376    15.3%\n",
      "------------------------------\n",
      "Total           35040\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BLOCK_DAYS = 7\n",
    "PATTERN = [0, 0, 0, 0, 1, 2]\n",
    "\n",
    "# Split\n",
    "train_df, val_df, test_df = block_time_series_split(\n",
    "    df, \n",
    "    block_days=BLOCK_DAYS, \n",
    "    pattern=PATTERN\n",
    ")\n",
    "\n",
    "# Summary\n",
    "total = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"{'Set':<12} {'Rows':>8} {'%':>8}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Train':<12} {len(train_df):>8} {100*len(train_df)/total:>7.1f}%\")\n",
    "print(f\"{'Validation':<12} {len(val_df):>8} {100*len(val_df)/total:>7.1f}%\")\n",
    "print(f\"{'Test':<12} {len(test_df):>8} {100*len(test_df)/total:>7.1f}%\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Total':<12} {total:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11eb7380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonal Coverage:\n",
      "==================================================\n",
      "\n",
      "Train:\n",
      "  Winter (Dec-Feb):  5943 rows (24.6%)\n",
      "  Spring (Mar-May):  6144 rows (25.4%)\n",
      "  Summer (Jun-Aug):  6144 rows (25.4%)\n",
      "  Autumn (Sep-Nov):  5961 rows (24.6%)\n",
      "\n",
      "Val:\n",
      "  Winter (Dec-Feb):  1344 rows (24.6%)\n",
      "  Spring (Mar-May):  1344 rows (24.6%)\n",
      "  Summer (Jun-Aug):  1344 rows (24.6%)\n",
      "  Autumn (Sep-Nov):  1440 rows (26.3%)\n",
      "\n",
      "Test:\n",
      "  Winter (Dec-Feb):  1353 rows (25.2%)\n",
      "  Spring (Mar-May):  1344 rows (25.0%)\n",
      "  Summer (Jun-Aug):  1344 rows (25.0%)\n",
      "  Autumn (Sep-Nov):  1335 rows (24.8%)\n"
     ]
    }
   ],
   "source": [
    "# Seasonal coverage check\n",
    "seasons = {\n",
    "    'Winter (Dec-Feb)': [12, 1, 2],\n",
    "    'Spring (Mar-May)': [3, 4, 5],\n",
    "    'Summer (Jun-Aug)': [6, 7, 8],\n",
    "    'Autumn (Sep-Nov)': [9, 10, 11]\n",
    "}\n",
    "\n",
    "print(\"Seasonal Coverage:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    months = pd.to_datetime(split_df['Time']).dt.month\n",
    "    print(f\"\\n{name}:\")\n",
    "    for season, month_list in seasons.items():\n",
    "        count = months.isin(month_list).sum()\n",
    "        pct = 100 * count / len(split_df)\n",
    "        print(f\"  {season}: {count:>5} rows ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7ec5e",
   "metadata": {},
   "source": [
    "## 2. Scaling (MinMax Normalization)\n",
    "\n",
    "**Process:**\n",
    "1. Fit scaler on Train set only (avoid data leakage)\n",
    "2. Transform Train, Val, Test with the same scaler\n",
    "3. Save scaler for inference\n",
    "\n",
    "**Output:** Values in range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a080a7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to scale: ['Aggregate', 'RangeHood', 'Dryer', 'Stove', 'GarageCabinet', 'ChargingStation_Socket', 'Oven', 'RainwaterPump', 'SmappeeCharger', 'Dishwasher', 'HeatPump', 'HeatPump_Controller', 'WashingMachine', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "# Columns to scale (exclude Time)\n",
    "feature_columns = [col for col in df.columns if col != 'Time']\n",
    "print(f\"Features to scale: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f769247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled shapes:\n",
      "  Train: (24192, 19)\n",
      "  Val:   (5472, 19)\n",
      "  Test:  (5376, 19)\n",
      "\n",
      "Value ranges after scaling:\n",
      "  Train: [0.0000, 1.0000]\n",
      "  Val:   [0.0000, 1.0000]\n",
      "  Test:  [0.0000, 1.2496]\n"
     ]
    }
   ],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on Train only\n",
    "scaler.fit(train_df[feature_columns])\n",
    "\n",
    "# Transform all sets\n",
    "train_scaled = scaler.transform(train_df[feature_columns])\n",
    "val_scaled = scaler.transform(val_df[feature_columns])\n",
    "test_scaled = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "print(f\"Scaled shapes:\")\n",
    "print(f\"  Train: {train_scaled.shape}\")\n",
    "print(f\"  Val:   {val_scaled.shape}\")\n",
    "print(f\"  Test:  {test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nValue ranges after scaling:\")\n",
    "print(f\"  Train: [{train_scaled.min():.4f}, {train_scaled.max():.4f}]\")\n",
    "print(f\"  Val:   [{val_scaled.min():.4f}, {val_scaled.max():.4f}]\")\n",
    "print(f\"  Test:  [{test_scaled.min():.4f}, {test_scaled.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f75c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler: C:\\Users\\Tommaso\\Documents\\MEGAR2D2\\HOWEST\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\\model_ready\\scaler.pkl\n",
      "\n",
      "Feature index mapping:\n",
      "  0: Aggregate\n",
      "  1: RangeHood\n",
      "  2: Dryer\n",
      "  3: Stove\n",
      "  4: GarageCabinet\n",
      "  5: ChargingStation_Socket\n",
      "  6: Oven\n",
      "  7: RainwaterPump\n",
      "  8: SmappeeCharger\n",
      "  9: Dishwasher\n",
      "  10: HeatPump\n",
      "  11: HeatPump_Controller\n",
      "  12: WashingMachine\n",
      "  13: hour_sin\n",
      "  14: hour_cos\n",
      "  15: dow_sin\n",
      "  16: dow_cos\n",
      "  17: month_sin\n",
      "  18: month_cos\n"
     ]
    }
   ],
   "source": [
    "# Save scaler for inference\n",
    "scaler_path = OUTPUT_DIR / 'scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler: {scaler_path}\")\n",
    "\n",
    "# Save feature names mapping\n",
    "feature_mapping = {i: col for i, col in enumerate(feature_columns)}\n",
    "print(f\"\\nFeature index mapping:\")\n",
    "for idx, name in feature_mapping.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab08745",
   "metadata": {},
   "source": [
    "## 3. Windowing (Sequence Generation)\n",
    "\n",
    "**Seq2Seq Architecture with Temporal Features:**\n",
    "- Input (X): Window of [Aggregate + 6 temporal features] → shape: (window_size, 7)\n",
    "- Output (y): Full sequence of appliance power values → shape: (window_size, 1)\n",
    "\n",
    "**Input Features:**\n",
    "- `Aggregate`: Total consumption (kW)\n",
    "- `hour_sin`, `hour_cos`: Daily pattern\n",
    "- `dow_sin`, `dow_cos`: Weekly pattern\n",
    "- `month_sin`, `month_cos`: Seasonal pattern\n",
    "\n",
    "**Window Configuration:**\n",
    "- Window size: 96 (exactly 24 hours at 15-min resolution)\n",
    "- Stride: 1 (sliding window)\n",
    "- Output: Entire appliance sequence (full day prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c544a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(\n",
    "    data: np.ndarray,\n",
    "    input_indices: list,\n",
    "    target_idx: int,\n",
    "    window_size: int = 99,\n",
    "    stride: int = 1\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for Seq2Seq NILM with temporal features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Scaled data array (n_samples, n_features)\n",
    "    input_indices : list\n",
    "        Column indices for input features (Aggregate + temporal features)\n",
    "    target_idx : int\n",
    "        Column index for target appliance (output)\n",
    "    window_size : int\n",
    "        Size of input/output windows\n",
    "    stride : int\n",
    "        Step between consecutive windows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (X, y) where X is (n_windows, window_size, n_input_features) and y is (n_windows, window_size, 1)\n",
    "    \"\"\"\n",
    "    n_samples = len(data)\n",
    "    n_input_features = len(input_indices)\n",
    "    \n",
    "    # Calculate number of valid windows\n",
    "    n_windows = (n_samples - window_size) // stride + 1\n",
    "    \n",
    "    # Pre-allocate arrays\n",
    "    X = np.zeros((n_windows, window_size, n_input_features), dtype=np.float32)\n",
    "    y = np.zeros((n_windows, window_size, 1), dtype=np.float32)\n",
    "    \n",
    "    # Generate windows\n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        # Input: Aggregate + temporal features window\n",
    "        X[i, :, :] = data[start_idx:end_idx, input_indices]\n",
    "        # Output: Full appliance sequence\n",
    "        y[i, :, 0] = data[start_idx:end_idx, target_idx]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0be1a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features (7):\n",
      "  0: Aggregate (original idx: 0)\n",
      "  1: hour_sin (original idx: 13)\n",
      "  2: hour_cos (original idx: 14)\n",
      "  3: dow_sin (original idx: 15)\n",
      "  4: dow_cos (original idx: 16)\n",
      "  5: month_sin (original idx: 17)\n",
      "  6: month_cos (original idx: 18)\n",
      "\n",
      "Available appliances (12):\n",
      "  1: RangeHood\n",
      "  2: Dryer\n",
      "  3: Stove\n",
      "  4: GarageCabinet\n",
      "  5: ChargingStation_Socket\n",
      "  6: Oven\n",
      "  7: RainwaterPump\n",
      "  8: SmappeeCharger\n",
      "  9: Dishwasher\n",
      "  10: HeatPump\n",
      "  11: HeatPump_Controller\n",
      "  12: WashingMachine\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "WINDOW_SIZE = 96  # Exactly 24 hours at 15-min resolution (optimal for Seq2Seq)\n",
    "STRIDE = 1\n",
    "\n",
    "# Define input features (Aggregate + temporal)\n",
    "TEMPORAL_COLS = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
    "INPUT_COLS = ['Aggregate'] + TEMPORAL_COLS\n",
    "INPUT_INDICES = [feature_columns.index(col) for col in INPUT_COLS]\n",
    "\n",
    "print(f\"Input features ({len(INPUT_COLS)}):\")\n",
    "for i, col in enumerate(INPUT_COLS):\n",
    "    idx = feature_columns.index(col)\n",
    "    print(f\"  {i}: {col} (original idx: {idx})\")\n",
    "\n",
    "# Available appliances for training (exclude Aggregate and temporal)\n",
    "appliance_columns = [col for col in feature_columns if col not in ['Aggregate'] + TEMPORAL_COLS]\n",
    "print(f\"\\nAvailable appliances ({len(appliance_columns)}):\")\n",
    "for col in appliance_columns:\n",
    "    idx = feature_columns.index(col)\n",
    "    print(f\"  {idx}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25d4ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target appliance: HeatPump (index 10)\n",
      "Window size: 96 samples (~24.0 hours)\n"
     ]
    }
   ],
   "source": [
    "# Select target appliance for this run\n",
    "# Change this to train for different appliances\n",
    "TARGET_APPLIANCE = 'HeatPump'  # Example: high-consumption, good variance\n",
    "\n",
    "TARGET_IDX = feature_columns.index(TARGET_APPLIANCE)\n",
    "print(f\"Target appliance: {TARGET_APPLIANCE} (index {TARGET_IDX})\")\n",
    "print(f\"Window size: {WINDOW_SIZE} samples (~{WINDOW_SIZE * 15 / 60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d2b4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequences...\n",
      "Train: X=(24097, 96, 7), y=(24097, 96, 1)\n",
      "Val:   X=(5377, 96, 7), y=(5377, 96, 1)\n",
      "Test:  X=(5281, 96, 7), y=(5281, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate sequences for each set\n",
    "print(\"Generating sequences...\")\n",
    "\n",
    "X_train, y_train = create_sequences(\n",
    "    train_scaled, INPUT_INDICES, TARGET_IDX, WINDOW_SIZE, STRIDE\n",
    ")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "X_val, y_val = create_sequences(\n",
    "    val_scaled, INPUT_INDICES, TARGET_IDX, WINDOW_SIZE, STRIDE\n",
    ")\n",
    "print(f\"Val:   X={X_val.shape}, y={y_val.shape}\")\n",
    "\n",
    "X_test, y_test = create_sequences(\n",
    "    test_scaled, INPUT_INDICES, TARGET_IDX, WINDOW_SIZE, STRIDE\n",
    ")\n",
    "print(f\"Test:  X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75835741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence verification:\n",
      "  X dtype: float32\n",
      "  y dtype: float32\n",
      "  X range: [0.0000, 1.0000]\n",
      "  y range: [0.0000, 1.0000]\n",
      "\n",
      "Sample window (first sequence):\n",
      "  Input shape: (96, 7) (7 features)\n",
      "    [0] Aggregate: range [0.015, 0.600]\n",
      "    [1] hour_sin: range [0.000, 1.000]\n",
      "    [2] hour_cos: range [0.000, 1.000]\n",
      "    [3] dow_sin: range [0.099, 0.500]\n",
      "    [4] dow_cos: range [0.802, 1.000]\n",
      "    [5] month_sin: range [0.000, 0.000]\n",
      "    [6] month_cos: range [0.500, 0.500]\n",
      "  Output (HeatPump): shape (96, 1)\n"
     ]
    }
   ],
   "source": [
    "# Verify sequence structure\n",
    "print(\"Sequence verification:\")\n",
    "print(f\"  X dtype: {X_train.dtype}\")\n",
    "print(f\"  y dtype: {y_train.dtype}\")\n",
    "print(f\"  X range: [{X_train.min():.4f}, {X_train.max():.4f}]\")\n",
    "print(f\"  y range: [{y_train.min():.4f}, {y_train.max():.4f}]\")\n",
    "\n",
    "# Sample window visualization\n",
    "print(f\"\\nSample window (first sequence):\")\n",
    "print(f\"  Input shape: {X_train[0].shape} ({len(INPUT_COLS)} features)\")\n",
    "for i, col in enumerate(INPUT_COLS):\n",
    "    print(f\"    [{i}] {col}: range [{X_train[0, :, i].min():.3f}, {X_train[0, :, i].max():.3f}]\")\n",
    "print(f\"  Output ({TARGET_APPLIANCE}): shape {y_train[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83219e",
   "metadata": {},
   "source": [
    "## 4. Shuffling (Training Set Only)\n",
    "\n",
    "**Why shuffle?**\n",
    "- Break temporal correlation between consecutive batches\n",
    "- Stabilize gradient descent\n",
    "- Prevent the model from learning sequence order\n",
    "\n",
    "**Note:** Only shuffle Train set. Val/Test remain in temporal order for proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11b286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled 24097 training samples\n",
      "First 5 shuffle indices: [ 9816 10054 17984 12796 23762]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle training data\n",
    "np.random.seed(42)  # Reproducibility\n",
    "\n",
    "shuffle_idx = np.random.permutation(len(X_train))\n",
    "X_train_shuffled = X_train[shuffle_idx]\n",
    "y_train_shuffled = y_train[shuffle_idx]\n",
    "\n",
    "print(f\"Shuffled {len(X_train)} training samples\")\n",
    "print(f\"First 5 shuffle indices: {shuffle_idx[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d3465",
   "metadata": {},
   "source": [
    "## 5. Export (Save Tensors)\n",
    "\n",
    "Save prepared data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc54a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\Users\\Tommaso\\Documents\\MEGAR2D2\\HOWEST\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\\model_ready\\heatpump\n",
      "\n",
      "Files:\n",
      "  X_test.npy: 13.54 MB\n",
      "  X_train.npy: 61.77 MB\n",
      "  X_val.npy: 13.78 MB\n",
      "  y_test.npy: 1.93 MB\n",
      "  y_train.npy: 8.82 MB\n",
      "  y_val.npy: 1.97 MB\n"
     ]
    }
   ],
   "source": [
    "# Create appliance-specific directory\n",
    "appliance_dir = OUTPUT_DIR / TARGET_APPLIANCE.lower()\n",
    "appliance_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as numpy arrays\n",
    "np.save(appliance_dir / 'X_train.npy', X_train_shuffled)\n",
    "np.save(appliance_dir / 'y_train.npy', y_train_shuffled)\n",
    "np.save(appliance_dir / 'X_val.npy', X_val)\n",
    "np.save(appliance_dir / 'y_val.npy', y_val)\n",
    "np.save(appliance_dir / 'X_test.npy', X_test)\n",
    "np.save(appliance_dir / 'y_test.npy', y_test)\n",
    "\n",
    "print(f\"Saved to: {appliance_dir}\")\n",
    "print(f\"\\nFiles:\")\n",
    "for f in sorted(appliance_dir.glob('*.npy')):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c794f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved:\n",
      "  target_appliance: HeatPump\n",
      "  window_size: 96\n",
      "  stride: 1\n",
      "  input_columns: ['Aggregate', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
      "  input_indices: [0, 13, 14, 15, 16, 17, 18]\n",
      "  target_idx: 10\n",
      "  feature_columns: ['Aggregate', 'RangeHood', 'Dryer', 'Stove', 'GarageCabinet', 'ChargingStation_Socket', 'Oven', 'RainwaterPump', 'SmappeeCharger', 'Dishwasher', 'HeatPump', 'HeatPump_Controller', 'WashingMachine', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
      "  n_input_features: 7\n",
      "  train_shape: (24097, 96, 7)\n",
      "  val_shape: (5377, 96, 7)\n",
      "  test_shape: (5281, 96, 7)\n",
      "  scaler_path: C:\\Users\\gamek\\School\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\\model_ready\\scaler.pkl\n",
      "  block_days: 7\n",
      "  split_pattern: [0, 0, 0, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'target_appliance': TARGET_APPLIANCE,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'input_columns': INPUT_COLS,\n",
    "    'input_indices': INPUT_INDICES,\n",
    "    'target_idx': TARGET_IDX,\n",
    "    'feature_columns': feature_columns,\n",
    "    'n_input_features': len(INPUT_COLS),\n",
    "    'train_shape': X_train_shuffled.shape,\n",
    "    'val_shape': X_val.shape,\n",
    "    'test_shape': X_test.shape,\n",
    "    'scaler_path': str(scaler_path),\n",
    "    'block_days': BLOCK_DAYS,\n",
    "    'split_pattern': PATTERN\n",
    "}\n",
    "\n",
    "with open(appliance_dir / 'metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Metadata saved:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9219e4e",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15cf10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NILM PRE-TRAINING PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "1. SPLIT (Block Time-Series Interleaved)\n",
      "   Block size: 7 days\n",
      "   Train: 24192 rows (69.0%)\n",
      "   Val:   5472 rows (15.6%)\n",
      "   Test:  5376 rows (15.3%)\n",
      "\n",
      "2. SCALING (MinMax [0, 1])\n",
      "   Fitted on: Train set\n",
      "   Features: 19\n",
      "\n",
      "3. WINDOWING (Seq2Seq with Temporal Features)\n",
      "   Window size: 96 samples (~24.0 hours)\n",
      "   Input features: 7 (Aggregate + 6 temporal)\n",
      "   Target: HeatPump\n",
      "   X shape: (n_samples, 96, 7)\n",
      "   y shape: (n_samples, 96, 1)\n",
      "\n",
      "4. SHUFFLING\n",
      "   Train set: Shuffled\n",
      "   Val/Test: Temporal order preserved\n",
      "\n",
      "5. OUTPUT\n",
      "   Directory: C:\\Users\\gamek\\School\\TeamProject\\MTS3-MCTE-Team-Project-Energy-G1\\data\\processed\\15min\\model_ready\\heatpump\n",
      "   X_train: (24097, 96, 7)\n",
      "   X_val:   (5377, 96, 7)\n",
      "   X_test:  (5281, 96, 7)\n",
      "\n",
      "============================================================\n",
      "Ready for model training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NILM PRE-TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. SPLIT (Block Time-Series Interleaved)\")\n",
    "print(f\"   Block size: {BLOCK_DAYS} days\")\n",
    "print(f\"   Train: {len(train_df)} rows ({100*len(train_df)/total:.1f}%)\")\n",
    "print(f\"   Val:   {len(val_df)} rows ({100*len(val_df)/total:.1f}%)\")\n",
    "print(f\"   Test:  {len(test_df)} rows ({100*len(test_df)/total:.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. SCALING (MinMax [0, 1])\")\n",
    "print(f\"   Fitted on: Train set\")\n",
    "print(f\"   Features: {len(feature_columns)}\")\n",
    "\n",
    "print(f\"\\n3. WINDOWING (Seq2Seq with Temporal Features)\")\n",
    "print(f\"   Window size: {WINDOW_SIZE} samples (~{WINDOW_SIZE * 15 / 60:.1f} hours)\")\n",
    "print(f\"   Input features: {len(INPUT_COLS)} (Aggregate + 6 temporal)\")\n",
    "print(f\"   Target: {TARGET_APPLIANCE}\")\n",
    "print(f\"   X shape: (n_samples, {WINDOW_SIZE}, {len(INPUT_COLS)})\")\n",
    "print(f\"   y shape: (n_samples, {WINDOW_SIZE}, 1)\")\n",
    "\n",
    "print(f\"\\n4. SHUFFLING\")\n",
    "print(f\"   Train set: Shuffled\")\n",
    "print(f\"   Val/Test: Temporal order preserved\")\n",
    "\n",
    "print(f\"\\n5. OUTPUT\")\n",
    "print(f\"   Directory: {appliance_dir}\")\n",
    "print(f\"   X_train: {X_train_shuffled.shape}\")\n",
    "print(f\"   X_val:   {X_val.shape}\")\n",
    "print(f\"   X_test:  {X_test.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ready for model training!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d5012",
   "metadata": {},
   "source": [
    "## 7. Generate for All Appliances (Optional)\n",
    "\n",
    "Run this cell to generate training data for all appliances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2f81e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_appliances(\n",
    "    train_scaled, val_scaled, test_scaled,\n",
    "    feature_columns, input_cols, output_dir, window_size=99, stride=1\n",
    "):\n",
    "    \"\"\"Generate training data for all appliances with temporal features.\"\"\"\n",
    "    \n",
    "    # Get input indices\n",
    "    input_indices = [feature_columns.index(col) for col in input_cols]\n",
    "    \n",
    "    # Define temporal columns to exclude from appliances\n",
    "    temporal_cols = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
    "    appliances = [col for col in feature_columns if col not in ['Aggregate'] + temporal_cols]\n",
    "    \n",
    "    for appliance in appliances:\n",
    "        print(f\"\\nProcessing: {appliance}\")\n",
    "        \n",
    "        target_idx = feature_columns.index(appliance)\n",
    "        app_dir = output_dir / appliance.lower()\n",
    "        app_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate sequences\n",
    "        X_train, y_train = create_sequences(\n",
    "            train_scaled, input_indices, target_idx, window_size, stride\n",
    "        )\n",
    "        X_val, y_val = create_sequences(\n",
    "            val_scaled, input_indices, target_idx, window_size, stride\n",
    "        )\n",
    "        X_test, y_test = create_sequences(\n",
    "            test_scaled, input_indices, target_idx, window_size, stride\n",
    "        )\n",
    "        \n",
    "        # Shuffle train\n",
    "        shuffle_idx = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffle_idx]\n",
    "        y_train = y_train[shuffle_idx]\n",
    "        \n",
    "        # Save\n",
    "        np.save(app_dir / 'X_train.npy', X_train)\n",
    "        np.save(app_dir / 'y_train.npy', y_train)\n",
    "        np.save(app_dir / 'X_val.npy', X_val)\n",
    "        np.save(app_dir / 'y_val.npy', y_val)\n",
    "        np.save(app_dir / 'X_test.npy', X_test)\n",
    "        np.save(app_dir / 'y_test.npy', y_test)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'target_appliance': appliance,\n",
    "            'window_size': window_size,\n",
    "            'stride': stride,\n",
    "            'input_columns': input_cols,\n",
    "            'n_input_features': len(input_cols),\n",
    "            'train_shape': X_train.shape,\n",
    "            'val_shape': X_val.shape,\n",
    "            'test_shape': X_test.shape\n",
    "        }\n",
    "        with open(app_dir / 'metadata.pkl', 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"  Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    print(f\"\\nAll appliances processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af6b2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: RangeHood\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: Dryer\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: Stove\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: GarageCabinet\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: ChargingStation_Socket\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: Oven\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: RainwaterPump\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: SmappeeCharger\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: Dishwasher\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: HeatPump\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: HeatPump_Controller\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "Processing: WashingMachine\n",
      "  Train: (24097, 96, 7), Val: (5377, 96, 7), Test: (5281, 96, 7)\n",
      "\n",
      "All appliances processed!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to generate for all appliances\n",
    "generate_all_appliances(\n",
    "     train_scaled, val_scaled, test_scaled,\n",
    "     feature_columns, INPUT_COLS, OUTPUT_DIR, WINDOW_SIZE, STRIDE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
