{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51481bdf",
   "metadata": {},
   "source": [
    "# NILM Pretraining - New 1-Second Data\n",
    "---\n",
    "\n",
    "This notebook prepares model-ready numpy arrays from the preprocessed **new 1-second data** (1sec_new).\n",
    "\n",
    "## Key Differences from Original Pretraining:\n",
    "1. **More appliances**: 11 targets vs 8 (added EVCharger, EVSocket, GarageCabinet)\n",
    "2. **Energy flow columns**: Solar, Grid, Battery as additional inputs\n",
    "3. **Larger dataset**: ~5.5M rows vs ~480K (21 months vs 3 months)\n",
    "4. **Chronological split**: Uses last months for validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29030d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e09cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('.').resolve().parent\n",
    "DATA_DIR = BASE_DIR / 'data' / 'processed' / '1sec_processed'\n",
    "OUTPUT_DIR = DATA_DIR / 'model_ready'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "input_file = DATA_DIR / 'nilm_ready_dataset_new.parquet'\n",
    "if not input_file.exists():\n",
    "    input_file = DATA_DIR / 'nilm_ready_dataset_new.csv'\n",
    "    \n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {input_file}\\n\"\n",
    "        \"Please run 'data_preparation_1sec_new.ipynb' first!\"\n",
    "    )\n",
    "\n",
    "if input_file.suffix == '.parquet':\n",
    "    df = pd.read_parquet(input_file)\n",
    "else:\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df = df.sort_values('Time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print(f\"Time range: {df['Time'].min()} \u2192 {df['Time'].max()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799252ac",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48309bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Resolution\n",
    "RESOLUTION_SEC = 1\n",
    "SAMPLES_PER_HOUR = 3600 // RESOLUTION_SEC  # 3600\n",
    "SAMPLES_PER_DAY = 24 * SAMPLES_PER_HOUR    # 86400\n",
    "\n",
    "# Window size (for Transformer input)\n",
    "# 1 hour = 360 samples at 10sec resolution\n",
    "WINDOW_SIZE = 600  # 10 minutes window\n",
    "STRIDE = 60  # 1 minute stride\n",
    "\n",
    "# Chronological split for new data\n",
    "# Train: March 2024 - September 2025 (18 months)\n",
    "# Val: October - November 2025 (2 months)\n",
    "# Test: December 2025 (partial)\n",
    "TRAIN_END = '2025-10-01'\n",
    "VAL_END = '2025-12-01'\n",
    "\n",
    "# All available appliances in new data\n",
    "APPLIANCE_COLUMNS = [\n",
    "    'HeatPump', 'Dishwasher', 'WashingMachine', 'Dryer',\n",
    "    'Oven', 'Stove', 'RangeHood', 'EVCharger', 'EVSocket',\n",
    "    'GarageCabinet', 'RainwaterPump'\n",
    "]\n",
    "\n",
    "# Energy flow columns (optional inputs)\n",
    "ENERGY_FLOW_COLUMNS = ['Solar', 'Grid', 'Battery']\n",
    "\n",
    "# Temporal features\n",
    "TEMPORAL_FEATURES = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']\n",
    "\n",
    "# Input configuration options\n",
    "USE_ENERGY_FLOW = True  # Include Solar/Grid/Battery as inputs\n",
    "\n",
    "if USE_ENERGY_FLOW:\n",
    "    INPUT_FEATURES = ['Aggregate'] + [c for c in ENERGY_FLOW_COLUMNS if c in df.columns] + TEMPORAL_FEATURES\n",
    "else:\n",
    "    INPUT_FEATURES = ['Aggregate'] + TEMPORAL_FEATURES\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Resolution: {RESOLUTION_SEC}sec\")\n",
    "print(f\"  Samples/day: {SAMPLES_PER_DAY}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE} ({WINDOW_SIZE * RESOLUTION_SEC / 3600:.1f} hours)\")\n",
    "print(f\"  Input features: {len(INPUT_FEATURES)} \u2192 {INPUT_FEATURES}\")\n",
    "print(f\"  Target appliances: {len(APPLIANCE_COLUMNS)}\")\n",
    "print(f\"  Use energy flow: {USE_ENERGY_FLOW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ecf053",
   "metadata": {},
   "source": [
    "## 2. Chronological Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to existing appliance columns\n",
    "existing_appliances = [c for c in APPLIANCE_COLUMNS if c in df.columns]\n",
    "print(f\"Existing appliances ({len(existing_appliances)}): {existing_appliances}\")\n",
    "\n",
    "# Check for any missing\n",
    "missing = set(APPLIANCE_COLUMNS) - set(existing_appliances)\n",
    "if missing:\n",
    "    print(f\"\u26a0\ufe0f Missing appliances (will be excluded): {missing}\")\n",
    "    APPLIANCE_COLUMNS = existing_appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ed4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological split\n",
    "print('='*70)\n",
    "print('\ud83d\udcc5 CHRONOLOGICAL SPLIT')\n",
    "print('='*70)\n",
    "\n",
    "train_mask = df['Time'] < TRAIN_END\n",
    "val_mask = (df['Time'] >= TRAIN_END) & (df['Time'] < VAL_END)\n",
    "test_mask = df['Time'] >= VAL_END\n",
    "\n",
    "train_df = df[train_mask].copy()\n",
    "val_df = df[val_mask].copy()\n",
    "test_df = df[test_mask].copy()\n",
    "\n",
    "# Summary\n",
    "total = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"\\n{'Set':<12} {'Rows':>12} {'%':>8} {'Days':>8} {'Time Range'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Train':<12} {len(train_df):>12,} {100*len(train_df)/total:>7.1f}% {len(train_df)/SAMPLES_PER_DAY:>7.1f}  {train_df['Time'].min().date()} \u2192 {train_df['Time'].max().date()}\")\n",
    "print(f\"{'Validation':<12} {len(val_df):>12,} {100*len(val_df)/total:>7.1f}% {len(val_df)/SAMPLES_PER_DAY:>7.1f}  {val_df['Time'].min().date()} \u2192 {val_df['Time'].max().date()}\")\n",
    "print(f\"{'Test':<12} {len(test_df):>12,} {100*len(test_df)/total:>7.1f}% {len(test_df)/SAMPLES_PER_DAY:>7.1f}  {test_df['Time'].min().date()} \u2192 {test_df['Time'].max().date()}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Total':<12} {total:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae426d",
   "metadata": {},
   "source": [
    "## 3. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to scale (all except Time)\n",
    "feature_columns = [col for col in df.columns if col != 'Time']\n",
    "print(f\"Features to scale ({len(feature_columns)}): {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "# Using MinMaxScaler for positive-only appliances\n",
    "# Could also use StandardScaler for Solar/Grid/Battery with negatives\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit on Train only (prevent data leakage)\n",
    "scaler.fit(train_df[feature_columns])\n",
    "\n",
    "# Transform all sets\n",
    "train_scaled = scaler.transform(train_df[feature_columns])\n",
    "val_scaled = scaler.transform(val_df[feature_columns])\n",
    "test_scaled = scaler.transform(test_df[feature_columns])\n",
    "\n",
    "print(f\"Scaled shapes:\")\n",
    "print(f\"  Train: {train_scaled.shape}\")\n",
    "print(f\"  Val:   {val_scaled.shape}\")\n",
    "print(f\"  Test:  {test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nValue ranges after scaling:\")\n",
    "print(f\"  Train: [{train_scaled.min():.4f}, {train_scaled.max():.4f}]\")\n",
    "print(f\"  Val:   [{val_scaled.min():.4f}, {val_scaled.max():.4f}]\")\n",
    "print(f\"  Test:  [{test_scaled.min():.4f}, {test_scaled.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler\n",
    "scaler_path = OUTPUT_DIR / 'scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler: {scaler_path}\")\n",
    "\n",
    "# Save feature names mapping\n",
    "feature_mapping = {i: col for i, col in enumerate(feature_columns)}\n",
    "print(f\"\\nFeature index mapping:\")\n",
    "for idx, name in feature_mapping.items():\n",
    "    print(f\"  {idx}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44b421",
   "metadata": {},
   "source": [
    "## 4. Windowing (Sequence Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5620bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(\n",
    "    data: np.ndarray,\n",
    "    window_size: int,\n",
    "    input_indices: list,\n",
    "    target_indices: list,\n",
    "    stride: int = 1\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for Transformer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Scaled data array (samples, features)\n",
    "    window_size : int\n",
    "        Number of time steps per sequence\n",
    "    input_indices : list\n",
    "        Column indices for input features\n",
    "    target_indices : list\n",
    "        Column indices for target appliances\n",
    "    stride : int\n",
    "        Step size between windows\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (X, y) arrays\n",
    "        X shape: (n_samples, window_size, n_input_features)\n",
    "        y shape: (n_samples, window_size, n_appliances)\n",
    "    \"\"\"\n",
    "    n_samples = (len(data) - window_size) // stride + 1\n",
    "    \n",
    "    X = np.zeros((n_samples, window_size, len(input_indices)), dtype=np.float32)\n",
    "    y = np.zeros((n_samples, window_size, len(target_indices)), dtype=np.float32)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        start = i * stride\n",
    "        end = start + window_size\n",
    "        X[i] = data[start:end, input_indices]\n",
    "        y[i] = data[start:end, target_indices]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ac929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column indices\n",
    "input_indices = [feature_columns.index(f) for f in INPUT_FEATURES if f in feature_columns]\n",
    "target_indices = [feature_columns.index(f) for f in APPLIANCE_COLUMNS if f in feature_columns]\n",
    "\n",
    "print(f\"Input features ({len(input_indices)}):\")\n",
    "for i in input_indices:\n",
    "    print(f\"  [{i}] {feature_columns[i]}\")\n",
    "\n",
    "print(f\"\\nTarget appliances ({len(target_indices)}):\")\n",
    "for i in target_indices:\n",
    "    print(f\"  [{i}] {feature_columns[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stride > 1 to reduce number of sequences (memory efficiency)\n",
    "# stride = 60 \u2192 sequences every 10 minutes (at 10sec resolution)\n",
    "STRIDE = 60\n",
    "\n",
    "print(f\"Creating sequences with window={WINDOW_SIZE}, stride={STRIDE}...\")\n",
    "print(f\"(One window every {STRIDE * RESOLUTION_SEC / 60:.0f} minutes)\")\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train = create_sequences(train_scaled, WINDOW_SIZE, input_indices, target_indices, stride=STRIDE)\n",
    "X_val, y_val = create_sequences(val_scaled, WINDOW_SIZE, input_indices, target_indices, stride=STRIDE)\n",
    "X_test, y_test = create_sequences(test_scaled, WINDOW_SIZE, input_indices, target_indices, stride=STRIDE)\n",
    "\n",
    "print(f\"\\nSequence shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} (samples, window, features)\")\n",
    "print(f\"  y_train: {y_train.shape} (samples, window, appliances)\")\n",
    "print(f\"  X_val:   {X_val.shape}\")\n",
    "print(f\"  y_val:   {y_val.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")\n",
    "\n",
    "# Memory estimate\n",
    "total_bytes = (X_train.nbytes + y_train.nbytes + X_val.nbytes + y_val.nbytes + X_test.nbytes + y_test.nbytes)\n",
    "print(f\"\\nTotal memory: {total_bytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49593c45",
   "metadata": {},
   "source": [
    "## 5. Verify Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74faa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a sample sequence\n",
    "sample_idx = 100\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Input (Aggregate)\n",
    "ax1 = axes[0]\n",
    "ax1.plot(X_train[sample_idx, :, 0], label='Aggregate (scaled)', linewidth=0.8)\n",
    "if len(input_indices) > 1:\n",
    "    # Also plot Solar/Grid if available\n",
    "    for i, idx in enumerate(input_indices[1:4]):\n",
    "        if i < X_train.shape[2] - 1:\n",
    "            ax1.plot(X_train[sample_idx, :, i+1], label=f'{feature_columns[idx]} (scaled)', alpha=0.7, linewidth=0.8)\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('Scaled value')\n",
    "ax1.set_title(f'Sample {sample_idx}: Input Sequence')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Targets - High power appliances\n",
    "ax2 = axes[1]\n",
    "high_power = ['HeatPump', 'EVCharger', 'GarageCabinet', 'Stove']\n",
    "target_names = [feature_columns[i] for i in target_indices]\n",
    "for i, name in enumerate(target_names):\n",
    "    if name in high_power:\n",
    "        ax2.plot(y_train[sample_idx, :, i], label=name, alpha=0.8, linewidth=0.8)\n",
    "ax2.set_xlabel('Time step')\n",
    "ax2.set_ylabel('Scaled value')\n",
    "ax2.set_title(f'Sample {sample_idx}: High-Power Appliances')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Targets - Low power appliances\n",
    "ax3 = axes[2]\n",
    "for i, name in enumerate(target_names):\n",
    "    if name not in high_power:\n",
    "        ax3.plot(y_train[sample_idx, :, i], label=name, alpha=0.8, linewidth=0.8)\n",
    "ax3.set_xlabel('Time step')\n",
    "ax3.set_ylabel('Scaled value')\n",
    "ax3.set_title(f'Sample {sample_idx}: Other Appliances')\n",
    "ax3.legend(bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ff4eb",
   "metadata": {},
   "source": [
    "## 6. Export Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('\ud83d\udcbe EXPORTING NUMPY ARRAYS')\n",
    "print('='*70)\n",
    "\n",
    "# Save arrays\n",
    "np.save(OUTPUT_DIR / 'X_train.npy', X_train)\n",
    "np.save(OUTPUT_DIR / 'y_train.npy', y_train)\n",
    "np.save(OUTPUT_DIR / 'X_val.npy', X_val)\n",
    "np.save(OUTPUT_DIR / 'y_val.npy', y_val)\n",
    "np.save(OUTPUT_DIR / 'X_test.npy', X_test)\n",
    "np.save(OUTPUT_DIR / 'y_test.npy', y_test)\n",
    "\n",
    "print(f'\\n\u2705 Arrays saved to {OUTPUT_DIR}:')\n",
    "for f in OUTPUT_DIR.glob('*.npy'):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f'   {f.name}: {size_mb:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'resolution_sec': RESOLUTION_SEC,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'input_features': [feature_columns[i] for i in input_indices],\n",
    "    'target_appliances': [feature_columns[i] for i in target_indices],\n",
    "    'feature_columns': feature_columns,\n",
    "    'use_energy_flow': USE_ENERGY_FLOW,\n",
    "    'split': {\n",
    "        'method': 'chronological',\n",
    "        'train_end': TRAIN_END,\n",
    "        'val_end': VAL_END\n",
    "    },\n",
    "    'shapes': {\n",
    "        'X_train': list(X_train.shape),\n",
    "        'y_train': list(y_train.shape),\n",
    "        'X_val': list(X_val.shape),\n",
    "        'y_val': list(y_val.shape),\n",
    "        'X_test': list(X_test.shape),\n",
    "        'y_test': list(y_test.shape)\n",
    "    },\n",
    "    'data_source': 'new 1sec data (1sec_new)',\n",
    "    'preprocessing_notes': [\n",
    "        'Applied clip(lower=0) for sensor offset removal (NOT abs!)',\n",
    "        'Kept negative values for Solar, Grid, Battery',\n",
    "        'NO resampling - kept native 1-second resolution',\n",
    "        'Chronological split (no data leakage)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# Also save as JSON for easy reading\n",
    "import json\n",
    "with open(OUTPUT_DIR / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f'\\n\u2705 Metadata saved')\n",
    "print(f'\\nMetadata summary:')\n",
    "print(f'  Input features: {len(metadata[\"input_features\"])} \u2192 {metadata[\"input_features\"]}')\n",
    "print(f'  Target appliances: {len(metadata[\"target_appliances\"])} \u2192 {metadata[\"target_appliances\"]}')\n",
    "print(f'  Training sequences: {X_train.shape[0]:,}')\n",
    "print(f'  Validation sequences: {X_val.shape[0]:,}')\n",
    "print(f'  Test sequences: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310afe4",
   "metadata": {},
   "source": [
    "---\n",
    "## \u2705 Pretraining Complete\n",
    "\n",
    "### Output Summary\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| X_train.npy | Train input sequences |\n",
    "| y_train.npy | Train target sequences |\n",
    "| X_val.npy | Validation input |\n",
    "| y_val.npy | Validation target |\n",
    "| X_test.npy | Test input |\n",
    "| y_test.npy | Test target |\n",
    "| scaler.pkl | MinMaxScaler for inference |\n",
    "| metadata.pkl | Configuration and feature names |\n",
    "| metadata.json | Human-readable metadata |\n",
    "\n",
    "### Input Features (10 with energy flow)\n",
    "- Aggregate (total consumption)\n",
    "- Solar, Grid, Battery (energy flow)\n",
    "- hour_sin, hour_cos\n",
    "- dow_sin, dow_cos\n",
    "- month_sin, month_cos\n",
    "\n",
    "### Target Appliances (11)\n",
    "1. HeatPump\n",
    "2. Dishwasher\n",
    "3. WashingMachine\n",
    "4. Dryer\n",
    "5. Oven\n",
    "6. Stove\n",
    "7. RangeHood\n",
    "8. EVCharger\n",
    "9. EVSocket\n",
    "10. GarageCabinet\n",
    "11. RainwaterPump\n",
    "\n",
    "### Next Steps\n",
    "1. Train Transformer/CNN model on these sequences\n",
    "2. Compare with previous 3-month model\n",
    "3. Evaluate on held-out December 2025 test set"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}